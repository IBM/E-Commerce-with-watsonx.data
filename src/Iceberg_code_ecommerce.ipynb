{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "f905f0bd-1905-4753-a5a1-555940890528"
   },
   "outputs": [],
   "source": [
    "wxd_hms_endpoint='thrift://<YOUR WXD ENDPOINT>'\n",
    "wxd_hms_username='ibmlhapikey'\n",
    "wxd_hms_password='****'\n",
    "source_bucket_endpoint='s3.us-south.cloud-object-storage.appdomain.cloud'\n",
    "source_bucket_access_key='****'\n",
    "source_bucket_secret_key='****'\n",
    "wxd_bucket_endpoint='s3.us-south.cloud-object-storage.appdomain.cloud'\n",
    "wxd_bucket_access_key='****'\n",
    "wxd_bucket_secret_key='****'\n",
    "db_host_name = 'jdbc:db2://*HOSTNAME*:*PORTNUMBER*/*DBNAME*'\n",
    "db_user_name='******'\n",
    "db_password='****'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "msg_id": "77fc15a4-02eb-4ba0-9952-b3ec813a8f0d"
   },
   "outputs": [],
   "source": [
    "conf=spark.sparkContext.getConf()\n",
    "spark.stop()\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf.setAll([\n",
    "    (\"spark.sql.catalogImplementation\", \"hive\"),\n",
    "    (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"),\n",
    "    (\"spark.sql.iceberg.vectorization.enabled\", \"false\"),\n",
    "    (\"spark.sql.catalog.lakehouse\", \"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "    (\"spark.sql.catalog.lakehouse.type\", \"hive\"),\n",
    "    (\"spark.sql.catalog.lakehouse.uri\", wxd_hms_endpoint),\n",
    "    (\"spark.hive.metastore.client.auth.mode\", \"PLAIN\"),\n",
    "    (\"spark.hive.metastore.client.plain.username\", wxd_hms_username),\n",
    "    (\"spark.hive.metastore.client.plain.password\", wxd_hms_password),\n",
    "    (\"spark.hive.metastore.use.SSL\", \"true\"),\n",
    "    (\"spark.hive.metastore.truststore.type\", \"JKS\"),\n",
    "    (\"spark.hive.metastore.truststore.path\", \"file:///opt/ibm/jdk/lib/security/cacerts\"),\n",
    "    (\"spark.hive.metastore.truststore.password\", \"changeit\"),\n",
    "    (\"spark.hadoop.fs.s3a.bucket.sparkdb2-target.endpoint\", source_bucket_endpoint),\n",
    "    (\"spark.hadoop.fs.s3a.bucket.sparkdb2-target.key\", source_bucket_access_key),\n",
    "    (\"spark.hadoop.fs.s3a.bucket.sparkdb2-target.secret.key\", source_bucket_secret_key),\n",
    "    (\"spark.hadoop.fs.s3a.bucket.sparkdb2-target.endpoint\", wxd_bucket_endpoint),\n",
    "    (\"spark.hadoop.fs.s3a.bucket.sparkdb2-target.access.key\", wxd_bucket_access_key),\n",
    "    (\"spark.hadoop.fs.s3a.bucket.sparkdb2-target.secret.key\", wxd_bucket_secret_key),\n",
    "    # Add Db2 configurations here\n",
    "    (\"spark.sql.catalog.db2catalog.type\", \"jdbc\"),  # JDBC catalog type for Db2\n",
    "    (\"spark.sql.catalog.db2catalog.jdbc.url\", db_host_name),  # JDBC URL for Db2\n",
    "    (\"spark.sql.catalog.db2catalog.jdbc.user\", db_user_name),  # Username for Db2\n",
    "    (\"spark.sql.catalog.db2catalog.jdbc.password\", db_password),  # Password for Db2\n",
    "    (\"spark.sql.catalog.db2catalog.jdbc.sslConnection\", \"true\"),  # Enable SSL for JDBC connection\n",
    "    (\"spark.sql.catalog.db2catalog.jdbc.sslTrustStoreLocation\", \"s3a://*****\"),  # Path to the truststore file\n",
    "    (\"spark.sql.catalog.db2catalog.jdbc.sslTrustStorePassword\", \"*****\"),  # Password for the truststore\n",
    "    # Additional Db2 JDBC properties can be added here if needed\n",
    "])\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "msg_id": "a87987a2-f0af-40c5-b047-01b65ee7f172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          namespace|\n",
      "+-------------------+\n",
      "|`abyte-hive-schema`|\n",
      "|   bronze_deltalake|\n",
      "|          bronze_ec|\n",
      "|   bronze_ecommerce|\n",
      "|        bronze_hudi|\n",
      "|  bronze_sparkdelta|\n",
      "|bronze_sparkwithdb2|\n",
      "|        bronze_test|\n",
      "|               data|\n",
      "|            default|\n",
      "|             demodb|\n",
      "|            demodb1|\n",
      "|           demodb_1|\n",
      "|     gold_deltalake|\n",
      "|            gold_ec|\n",
      "|     gold_ecommerce|\n",
      "|          gold_hudi|\n",
      "|    gold_sparkdelta|\n",
      "|  gold_sparkwithdb2|\n",
      "|          gold_test|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def list_databases(spark):\n",
    "    # list the database under lakehouse catalog\n",
    "    spark.sql(\"show databases from lakehouse\").show() \n",
    "list_databases(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "msg_id": "f787c0a2-df0e-4cd4-9fcc-30a6eade4b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database bronze_ecommerce created at s3a://sparkdb2-target/bronze_ecom/\n",
      "Database silver_ecommerce created at s3a://sparkdb2-target/silver_ecom/\n",
      "Database gold_ecommerce created at s3a://sparkdb2-target/gold_ecom/\n"
     ]
    }
   ],
   "source": [
    "def create_databases(spark):\n",
    "    # Base S3 location\n",
    "    base_s3_path = \"s3a://sparkdb2-target/\"\n",
    "    \n",
    "    # Databases to be created with corresponding paths\n",
    "    databases = {\n",
    "        \"bronze_ecommerce\": f\"{base_s3_path}bronze_ecom/\",\n",
    "        \"silver_ecommerce\": f\"{base_s3_path}silver_ecom/\",\n",
    "        \"gold_ecommerce\": f\"{base_s3_path}gold_ecom/\"\n",
    "    }\n",
    "\n",
    "    # Loop through each database and create it if not exists\n",
    "    for db_name, db_location in databases.items():\n",
    "        try:\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS lakehouse.{db_name} LOCATION '{db_location}'\")\n",
    "            print(f\"Database {db_name} created at {db_location}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating database {db_name} at {db_location}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "create_databases(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "0d4934db-b06a-4ee8-a71e-a4814dc84113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Iceberg table: lakehouse.bronze_ecommerce.bronze_hist_sales at S3 path: s3a://sparkdb2-target/bronze_ecom/bronze_hist_sales\n",
      "Iceberg table lakehouse.bronze_ecommerce.bronze_hist_sales created successfully.\n"
     ]
    }
   ],
   "source": [
    "def migrate_db2_to_bronze(spark):\n",
    "    schema_name = \"ecommerce\"\n",
    "    # Assuming you have a list of table names in the BANKING schema\n",
    "    table_names = [\"HIST_SALES\"]  # Add more table names as needed\n",
    "\n",
    "    # Specify the base S3 path for the Iceberg tables\n",
    "    base_s3_path = \"s3a://sparkdb2-target/bronze_ecom/\"\n",
    "    \n",
    "    for table_name in table_names:\n",
    "        full_table_name = f\"{schema_name}.{table_name}\"\n",
    "\n",
    "        # Define the Iceberg table name with the bronze prefix\n",
    "        iceberg_table_name = f\"lakehouse.bronze_ecommerce.bronze_{table_name.lower()}\"  # Adding \"bronze_\" prefix\n",
    "        s3_table_path = f\"{base_s3_path}bronze_{table_name.lower()}\"  # S3 path must match the Iceberg table name\n",
    "\n",
    "        # Read data from DB2 Cloud into DataFrame\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_host_name) \\\n",
    "            .option(\"user\", db_user_name) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"dbtable\", full_table_name) \\\n",
    "            .option(\"sslConnection\", \"true\") \\\n",
    "            .load()\n",
    "\n",
    "        # Log the paths for debugging\n",
    "        print(f\"Creating Iceberg table: {iceberg_table_name} at S3 path: {s3_table_path}\")\n",
    "\n",
    "        # Create the Iceberg table in the Bronze layer with the specified S3 path\n",
    "        try:\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {iceberg_table_name} \n",
    "                USING iceberg \n",
    "                LOCATION '{s3_table_path}' \n",
    "                AS SELECT * FROM VALUES (1) WHERE FALSE\n",
    "            \"\"\")\n",
    "            print(f\"Iceberg table {iceberg_table_name} created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Iceberg table: {e}\")\n",
    "\n",
    "        # Write data to the Iceberg table\n",
    "        try:\n",
    "            df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(iceberg_table_name)\n",
    "            print(f\"Data written to Iceberg table {iceberg_table_name} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing data to Iceberg table: {e}\")\n",
    "\n",
    "        # Describe the Iceberg table\n",
    "        spark.sql(f'DESCRIBE TABLE {iceberg_table_name}').show(10)\n",
    "\n",
    "        # Query the Iceberg table\n",
    "        spark.sql(f'SELECT * FROM {iceberg_table_name}').show()\n",
    "\n",
    "        print(f\"Migration of table {iceberg_table_name} from DB2 to Watson Data Iceberg completed successfully.\")\n",
    "\n",
    "    print(\"All tables migration completed successfully.\")\n",
    "\n",
    "# Example usage\n",
    "migrate_db2_to_bronze(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "a7de5f72-df18-45ba-a0f4-fd4116ba4de0"
   },
   "outputs": [],
   "source": [
    "def bronze_ingest_from_csv_temp_table(spark):\n",
    "    # Load CSV data into a DataFrame\n",
    "    csvDF = spark.read.option(\"header\", True).csv(\"s3a://sparkdb2-target/promotions_sale.csv\")\n",
    "    csvDF.createOrReplaceTempView(\"promotions\")\n",
    "\n",
    "    # Specify the S3 path for the Iceberg table\n",
    "    s3_table_path = \"s3a://sparkdb2-target/bronze_ecom/bronze_promotions_sale\"\n",
    "\n",
    "    # Load temporary table into an Iceberg table with the specified S3 path\n",
    "    print(f\"Creating Iceberg table: lakehouse.bronze_ecommerce.bronze_promotions_sale\")\n",
    "    print(f\"Table will be saved at S3 path: {s3_table_path}\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE lakehouse.bronze_ecommerce.bronze_promotions_sale \n",
    "        USING iceberg \n",
    "        LOCATION '{s3_table_path}' \n",
    "        AS SELECT * FROM promotions\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Iceberg table created successfully.\")\n",
    "\n",
    "    # Describe the table created\n",
    "    print(\"Describing the created table:\")\n",
    "    spark.sql('DESCRIBE TABLE lakehouse.bronze_ecommerce.bronze_promotions_sale').show(10)\n",
    "    \n",
    "    # Query the table\n",
    "    print(\"Querying the created table:\")\n",
    "    spark.sql('SELECT * FROM lakehouse.bronze_ecommerce.bronze_promotions_sale').show()\n",
    "\n",
    "# Example usage\n",
    "bronze_ingest_from_csv_temp_table(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "7fa48843-a368-4796-bfa6-d4898602556a"
   },
   "outputs": [],
   "source": [
    "def create_enhanced_aggregated_sales_table(spark):\n",
    "    # Define the schema name and table names\n",
    "    schema_name = \"ecommerce\"\n",
    "    stock_table_name = \"STOCK\"  # Stock table in DB2\n",
    "    sales_table_name = \"HIST_SALES\"  # Assuming sales data is already migrated to Iceberg\n",
    "    accessory_table_name = \"ACCESSORIES\"  # Accessory table in DB2\n",
    "\n",
    "    # Load stock data from DB2\n",
    "    try:\n",
    "        stock_data = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_host_name) \\\n",
    "            .option(\"user\", db_user_name) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"dbtable\", f\"{schema_name}.{stock_table_name}\") \\\n",
    "            .option(\"sslConnection\", \"true\") \\\n",
    "            .load()\n",
    "        # Create a temporary view for stock data\n",
    "        stock_data.createOrReplaceTempView(\"stock_temp\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading stock data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load accessory data from DB2\n",
    "    try:\n",
    "        accessory_data = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_host_name) \\\n",
    "            .option(\"user\", db_user_name) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"dbtable\", f\"{schema_name}.{accessory_table_name}\") \\\n",
    "            .option(\"sslConnection\", \"true\") \\\n",
    "            .load()\n",
    "        # Create a temporary view for accessory data\n",
    "        accessory_data.createOrReplaceTempView(\"accessory_temp\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading accessory data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Create a temporary view from the sales table in the Bronze layer\n",
    "    sales_temp_view_query = f\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW sales_temp AS \n",
    "    SELECT * FROM lakehouse.bronze_ecommerce.bronze_{sales_table_name.lower()}\n",
    "    \"\"\"\n",
    "    spark.sql(sales_temp_view_query)\n",
    "\n",
    "    # Create a temporary view from the promotions table\n",
    "    promotions_temp_view_query = \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW promotions_temp AS \n",
    "    SELECT * FROM lakehouse.bronze_ecommerce.bronze_promotions_sale\n",
    "    \"\"\"\n",
    "    spark.sql(promotions_temp_view_query)\n",
    "\n",
    "    # Aggregating sales data for products and accessories\n",
    "    aggregated_sales_query = \"\"\"\n",
    "    SELECT\n",
    "        s.PRODUCT_ID,\n",
    "        DATE_FORMAT(s.SALE_DATE, 'yyyy-MM') AS sale_month,\n",
    "        COUNT(DISTINCT s.SALES_ID) AS total_sales_count,\n",
    "        SUM(s.TOTAL_AMOUNT) AS total_sales_amount,\n",
    "        AVG(s.TOTAL_AMOUNT) AS average_sales_amount,\n",
    "        MAX(p.Discount_Percentage) AS Discount_Percentage,\n",
    "        COUNT(st.PRODUCT_ID) AS total_stock_count,\n",
    "        CASE\n",
    "            WHEN COUNT(st.PRODUCT_ID) > 0 THEN 'In Stock'\n",
    "            ELSE 'Out of Stock'\n",
    "        END AS stock_status,\n",
    "        'Product' AS item_type\n",
    "    FROM sales_temp s\n",
    "    LEFT JOIN promotions_temp p ON s.PRODUCT_ID = p.Product_ID \n",
    "    LEFT JOIN stock_temp st ON s.PRODUCT_ID = st.PRODUCT_ID\n",
    "    WHERE s.PRODUCT_ID IS NOT NULL  -- Ensure PRODUCT_ID is not null\n",
    "    GROUP BY s.PRODUCT_ID, sale_month\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT\n",
    "        a.ACCESSORY_ID AS PRODUCT_ID,\n",
    "        DATE_FORMAT(s.SALE_DATE, 'yyyy-MM') AS sale_month,\n",
    "        COUNT(DISTINCT s.SALES_ID) AS total_sales_count,\n",
    "        SUM(s.TOTAL_AMOUNT) AS total_sales_amount,\n",
    "        AVG(s.TOTAL_AMOUNT) AS average_sales_amount,\n",
    "        MAX(p.Discount_Percentage) AS Discount_Percentage,\n",
    "        COUNT(st.ACCESSORY_ID) AS total_stock_count,\n",
    "        CASE\n",
    "            WHEN COUNT(st.ACCESSORY_ID) > 0 THEN 'In Stock'\n",
    "            ELSE 'Out of Stock'\n",
    "        END AS stock_status,\n",
    "        'Accessory' AS item_type\n",
    "    FROM sales_temp s\n",
    "    LEFT JOIN promotions_temp p ON s.ACCESSORY_ID = p.Product_ID \n",
    "    LEFT JOIN stock_temp st ON s.ACCESSORY_ID = st.ACCESSORY_ID\n",
    "    LEFT JOIN accessory_temp a ON s.ACCESSORY_ID = a.ACCESSORY_ID\n",
    "    WHERE s.IS_ACCESSORY = true  -- Filter for accessory sales\n",
    "    AND a.ACCESSORY_ID IS NOT NULL  -- Ensure ACCESSORY_ID is not null\n",
    "    GROUP BY a.ACCESSORY_ID, sale_month\n",
    "    ORDER BY sale_month\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the combined aggregation query\n",
    "    aggregated_sales_df = spark.sql(aggregated_sales_query)\n",
    "\n",
    "    # Create the Iceberg table for aggregated sales\n",
    "    aggregated_iceberg_table_name = \"lakehouse.silver_ecommerce.aggregated_sales_data\"\n",
    "    try:\n",
    "        aggregated_sales_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(aggregated_iceberg_table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the Iceberg table: {e}\")\n",
    "        return\n",
    "\n",
    "    # Describe the created Iceberg table for aggregated sales\n",
    "    spark.sql(f\"DESCRIBE TABLE {aggregated_iceberg_table_name}\").show()\n",
    "\n",
    "    # Query the Iceberg table to check the aggregated sales results\n",
    "    spark.sql(f\"SELECT * FROM {aggregated_iceberg_table_name} LIMIT 10\").show()  # Limit for easier viewing\n",
    "\n",
    "    print(\"Enhanced aggregation of product and accessory sales completed successfully.\")\n",
    "\n",
    "# Example usage\n",
    "create_enhanced_aggregated_sales_table(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "6a1f5aab-a6b3-4aa8-979d-63877e3097db"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def create_customer_segmentation_table(spark):\n",
    "    # Define the table names in the DB2 database\n",
    "    customer_table_name = \"ecommerce.customers\"\n",
    "    historical_sales_table_name = \"lakehouse.bronze_ecommerce.bronze_hist_sales\"  # Iceberg table for historical sales\n",
    "    promotions_table_name = \"lakehouse.bronze_ecommerce.bronze_promotions_sale\"  # Iceberg table for promotions\n",
    "    iceberg_table_name = \"lakehouse.silver_ecommerce.customer_segmentation\"\n",
    "\n",
    "    # Load customer data from DB2\n",
    "    print(f\"Loading customer data from {customer_table_name}...\")\n",
    "    customer_data = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", db_host_name) \\\n",
    "        .option(\"user\", db_user_name) \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .option(\"dbtable\", customer_table_name) \\\n",
    "        .option(\"sslConnection\", \"true\") \\\n",
    "        .load()\n",
    "    print(\"Customer data loaded successfully.\")\n",
    "\n",
    "    # Load historical sales data from Iceberg table\n",
    "    print(\"Loading historical sales data from Iceberg table...\")\n",
    "    sales_data = spark.sql(\"SELECT * FROM lakehouse.bronze_ecommerce.bronze_hist_sales\")\n",
    "    print(\"Historical sales data loaded successfully.\")\n",
    "\n",
    "    # Load promotions data from Iceberg table\n",
    "    print(\"Loading promotions data from Iceberg table...\")\n",
    "    promotions_data = spark.sql(\"SELECT * FROM lakehouse.bronze_ecommerce.bronze_promotions_sale\")\n",
    "    print(\"Promotions data loaded successfully.\")\n",
    "\n",
    "    # Join customer data with sales data on CUSTOMER_ID\n",
    "    print(\"Joining customer data with historical sales data...\")\n",
    "    joined_data = customer_data.join(sales_data, \"CUSTOMER_ID\", \"left\")\n",
    "    print(\"Customer data joined with historical sales data successfully.\")\n",
    "\n",
    "    # Optionally, join with promotions data if relevant\n",
    "    print(\"Joining with promotions data...\")\n",
    "    joined_data = joined_data.join(promotions_data, \"PRODUCT_ID\", \"left\")  # Adjust join condition as per your schema\n",
    "    print(\"Joined with promotions data successfully.\")\n",
    "\n",
    "    # Segment customers based on their total purchases and demographic data\n",
    "    print(\"Segmenting customers based on total purchases...\")\n",
    "    customer_segments = joined_data.groupBy(\"CUSTOMER_ID\", \"CUSTOMER_NAME\", \"LOCATION\") \\\n",
    "        .agg(\n",
    "            F.sum(col(\"TOTAL_AMOUNT\")).alias(\"TOTAL_PURCHASES\"),\n",
    "            F.avg(col(\"TOTAL_AMOUNT\")).alias(\"AVG_PURCHASE_AMOUNT\"),\n",
    "            F.count(\"*\").alias(\"NUM_PURCHASES\")\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"SEGMENT\",\n",
    "            when(col(\"TOTAL_PURCHASES\") > 1000, \"High Value\")\n",
    "            .when((col(\"TOTAL_PURCHASES\") >= 500) & (col(\"TOTAL_PURCHASES\") <= 1000), \"Medium Value\")\n",
    "            .otherwise(\"Low Value\")\n",
    "        )\n",
    "    print(\"Customer segmentation completed successfully.\")\n",
    "\n",
    "    # Check if the segmentation table exists\n",
    "    table_exists = False\n",
    "    try:\n",
    "        existing_segments = spark.table(iceberg_table_name)\n",
    "        print(f\"Customer segmentation table {iceberg_table_name} exists. Proceeding with upsert.\")\n",
    "        table_exists = True\n",
    "    except Exception as e:\n",
    "        print(f\"Customer segmentation table {iceberg_table_name} does not exist. Creating a new table.\")\n",
    "        table_exists = False\n",
    "\n",
    "    if table_exists:\n",
    "        # Perform upsert logic: update existing records and append new ones\n",
    "        print(\"Performing upsert operation...\")\n",
    "        updated_segments = existing_segments.alias(\"existing\").join(\n",
    "            customer_segments.alias(\"new\"),\n",
    "            \"CUSTOMER_ID\",\n",
    "            \"outer\"\n",
    "        ).select(\n",
    "            F.coalesce(col(\"new.CUSTOMER_ID\"), col(\"existing.CUSTOMER_ID\")).alias(\"CUSTOMER_ID\"),\n",
    "            F.coalesce(col(\"new.CUSTOMER_NAME\"), col(\"existing.CUSTOMER_NAME\")).alias(\"CUSTOMER_NAME\"),\n",
    "            F.coalesce(col(\"new.LOCATION\"), col(\"existing.LOCATION\")).alias(\"LOCATION\"),\n",
    "            F.coalesce(col(\"new.TOTAL_PURCHASES\"), col(\"existing.TOTAL_PURCHASES\")).alias(\"TOTAL_PURCHASES\"),\n",
    "            F.coalesce(col(\"new.AVG_PURCHASE_AMOUNT\"), col(\"existing.AVG_PURCHASE_AMOUNT\")).alias(\"AVG_PURCHASE_AMOUNT\"),\n",
    "            F.coalesce(col(\"new.NUM_PURCHASES\"), col(\"existing.NUM_PURCHASES\")).alias(\"NUM_PURCHASES\"),\n",
    "            F.coalesce(col(\"new.SEGMENT\"), col(\"existing.SEGMENT\")).alias(\"SEGMENT\")\n",
    "        )\n",
    "\n",
    "        # Write the updated segments back to the Iceberg table\n",
    "        updated_segments.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(iceberg_table_name)\n",
    "        print(\"Customer segmentation data upserted successfully.\")\n",
    "    else:\n",
    "        # If the table doesn't exist, create and insert new data\n",
    "        print(f\"Creating the table and inserting new data into {iceberg_table_name}...\")\n",
    "        customer_segments.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(iceberg_table_name)\n",
    "        print(f\"Customer segmentation table {iceberg_table_name} created and data inserted.\")\n",
    "\n",
    "    # Display the data for verification\n",
    "    print(\"Displaying the customer segmentation data for verification:\")\n",
    "    spark.sql(f\"SELECT * FROM {iceberg_table_name}\").show()\n",
    "\n",
    "# Example usage\n",
    "create_customer_segmentation_table(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
