{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "2f37b469-1813-4423-b1e2-444f641dd0cc"
   },
   "outputs": [],
   "source": [
    "## Watsonx.data Credentials\n",
    "UsernameCPD = 'ibmlhapikey' #The username for your watsonx.data instance. Here, ibmlhapikey.\n",
    "PasswordCPD = '****' #The API key generated for the watsonx.data\n",
    "\n",
    "##Cos bucket credentials\n",
    "LH_S3_ACCESS_KEY = \"****\"\n",
    "LH_S3_SECRET_KEY = \"****\" \n",
    "\n",
    "#DB2 details\n",
    "db_host_name = 'jdbc:db2://<DBHOSTNAME>'\n",
    "db_user_name='****'\n",
    "db_password='****'\n",
    "#####required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "msg_id": "2aa41ee8-0c56-4e19-ae29-0c1548a6bb7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.hive.metastore.truststore.password', 'changeit')\n",
      "('spark.hive.metastore.truststore.type', 'JKS')\n",
      "('spark.sql.catalog.db2catalog.jdbc.sslTrustStorePassword', '*****')\n",
      "('spark.network.crypto.keyLength', '256')\n",
      "('spark.driver.memory', '3600M')\n",
      "('spark.app.id', 'app-20241202130549-0001')\n",
      "('spark.network.crypto.enabled', 'true')\n",
      "('spark.hive.metastore.truststore.path', 'file:///opt/ibm/jdk/lib/security/cacerts')\n",
      "('spark.authenticate.enableSaslEncryption', 'true')\n",
      "('spark.ui.killEnabled', 'false')\n",
      "('spark.driver.host', 'spark-master-headless-bb3bfc7f-a72f-45e5-b047-d902e736dec6')\n",
      "('spark.app.submitTime', '1733144677576')\n",
      "('spark.r.command', '/home/spark/shared/R/bin/Rscript')\n",
      "('spark.hadoop.fs.stocator.glob.bracket.support', 'true')\n",
      "('spark.local.dir', '/tmp/spark/scratch')\n",
      "('spark.authenticate.secret', 'a1890ed2-bb98-479b-b127-ea9c295f44e6')\n",
      "('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
      "('spark.ui.reverseProxy', 'false')\n",
      "('spark.executor.memory', '3600M')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false  -Dderby.system.home=/home/spark/.local/share/jupyter/runtime/kernel-bb3bfc7f-a72f-45e5-b047-d902e736dec6-20241202_130424 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20241202_130424.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=false -Divy.cache.dir=/tmp -Divy.home=/tmp -Dfile.encoding=UTF-8 ')\n",
      "('spark.eventLog.enabled', 'false')\n",
      "('fs.s3a.path.style.access', 'true')\n",
      "('spark.eventLog.dir', 'file:///home/spark/shared/spark-events')\n",
      "('spark.blockManager.port', '6068')\n",
      "('spark.hadoop.fs.stocator.scheme.list', 'cos')\n",
      "('fs.s3a.secret.key', '**SECRET-KEY*')\n",
      "('spark.executor.extraClassPath', '/home/spark/shared/user-libs/spark/*:/home/spark/shared/user-libs/common/*:/home/spark/shared/user-libs/connectors/*:/opt/ibm/third-party/libs/spark2/*:/opt/ibm/third-party/libs/common/*:/opt/ibm/third-party/libs/connectors/*:/opt/ibm/connectors/parquet-encryption/*:/opt/ibm/connectors/db2/*:/opt/ibm/connectors/others-db-drivers/*:/opt/ibm/connectors/wdp-connector-driver/*:/opt/ibm/connectors/snowflake/*:/opt/ibm/connectors/data-engine/*:/opt/ibm/connectors/wxd/*:/opt/ibm/spark/external-jars/*')\n",
      "('spark.ui.enabled', 'true')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.port', '6067')\n",
      "('spark.sql.catalog.db2catalog.jdbc.password', '*DBPASSWORD*')\n",
      "('spark.sql.warehouse.dir', 'file:/home/spark/shared/spark-warehouse')\n",
      "('spark.sql.catalog.db2catalog.jdbc.user', '*DBUSER*')\n",
      "('spark.hadoop.fs.cos.impl', 'com.ibm.stocator.fs.ObjectStoreFileSystem')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false  -Dderby.system.home=/home/spark/.local/share/jupyter/runtime/kernel-bb3bfc7f-a72f-45e5-b047-d902e736dec6-20241202_130424 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20241202_130424.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=false --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Dfile.encoding=UTF-8 ')\n",
      "('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
      "('spark.jars.packages', 'io.delta:delta-core_2.12:2.2.0')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('fs.s3a.connection.ssl.enabled', 'true')\n",
      "('spark.master.ui.port', '8080')\n",
      "('spark.executor.instances', '1')\n",
      "('spark.history.fs.logDirectory', '/home/spark/shared/spark-event')\n",
      "('spark.hadoop.fs.s3a.fast.upload', 'true')\n",
      "('spark.authenticate', 'true')\n",
      "('spark.dynamicAllocation.enabled', 'false')\n",
      "('spark.app.startTime', '1733144749404')\n",
      "('spark.hive.metastore.client.auth.mode', 'PLAIN')\n",
      "('spark.shuffle.service.enabled', 'false')\n",
      "('spark.ui.https.enabled', 'true')\n",
      "('fs.s3a.access.key', '*ACCESSKEY*')\n",
      "('spark.ui.port', '4040')\n",
      "('spark.sql.catalog.db2catalog.type', 'jdbc')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.hive.metastore.use.SSL', 'true')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.driver.extraClassPath', '/home/spark/shared/user-libs/spark/*:/home/spark/shared/user-libs/common/*:/home/spark/shared/user-libs/connectors/*:/opt/ibm/third-party/libs/spark2/*:/opt/ibm/third-party/libs/common/*:/opt/ibm/third-party/libs/connectors/*:/opt/ibm/connectors/parquet-encryption/*:/opt/ibm/connectors/db2/*:/opt/ibm/connectors/others-db-drivers/*:/opt/ibm/connectors/wdp-connector-driver/*:/opt/ibm/connectors/snowflake/*:/opt/ibm/connectors/data-engine/*:/opt/ibm/connectors/wxd/*:/opt/ibm/spark/external-jars/*')\n",
      "('spark.app.name', 'python3.10')\n",
      "('spark.driver.cores', '1')\n",
      "('spark.sql.catalog.db2catalog.jdbc.sslTrustStoreLocation', 's3a://******')\n",
      "('spark.sql.catalog.db2catalog.jdbc.sslConnection', 'true')\n",
      "('spark.sql.catalog.db2catalog.jdbc.url', 'jdbc:db2://*HOSTNAME*:*PORT*/*DBNAME*')\n",
      "('spark.history.ui.port', '18080')\n",
      "('spark.worker.ui.port', '8081')\n",
      "('spark.shuffle.service.port', '7337')\n",
      "('spark.master', 'spark://jkg-deployment-bb3bfc7f-a72f-45e5-b047-d902e736dec6-5bf686q62vf:7077')\n",
      "('spark.hive.metastore.client.plain.password', '*METASTOREPWD*')\n",
      "('spark.hive.metastore.client.plain.username', 'ibmlhapikey')\n",
      "('spark.driver.bindAddress', '0.0.0.0')\n",
      "('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.hadoop.fs.stocator.cos.impl', 'com.ibm.stocator.fs.cos.COSAPIClient')\n",
      "('spark.ssl.ui.port', '4440')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.hadoop.fs.s3a.multipart.size', '33554432')\n",
      "('spark.executor.cores', '1')\n",
      "('fs.s3a.endpoint', 'http://s3.us-south.cloud-object-storage.appdomain.cloud')\n",
      "('spark.hadoop.fs.stocator.cos.scheme', 'cos')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
      "('spark.hive.metastore.uris', 'thrift://*METASTOREURL*')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('sparky').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "conf = sc.getConf()\n",
    "\n",
    "conf.set(\"spark.hive.metastore.uris\", f\"thrift://*WXDENDPOINT*\") \n",
    "conf.set(\"spark.hive.metastore.use.SSL\", \"true\")\n",
    "conf.set(\"spark.hive.metastore.truststore.type\", \"JKS\")\n",
    "conf.set(\"spark.hive.metastore.truststore.path\", \"file:///opt/ibm/jdk/lib/security/cacerts\")\n",
    "conf.set(\"spark.hive.metastore.truststore.password\", \"changeit\")\n",
    "\n",
    "conf.set(\"spark.hive.metastore.client.auth.mode\", \"PLAIN\")\n",
    "conf.set(\"spark.hive.metastore.client.plain.username\", UsernameCPD)\n",
    "conf.set(\"spark.hive.metastore.client.plain.password\", PasswordCPD)\n",
    "\n",
    "conf.set(\"fs.s3a.access.key\", LH_S3_ACCESS_KEY)\n",
    "conf.set(\"fs.s3a.secret.key\", LH_S3_SECRET_KEY)\n",
    "conf.set(\"fs.s3a.endpoint\", f\"http://s3.us-south.cloud-object-storage.appdomain.cloud\")\n",
    "\n",
    "conf.set(\"fs.s3a.connection.ssl.enabled\",  \"true\")\n",
    "conf.set(\"fs.s3a.path.style.access\",       \"true\")\n",
    "conf.set(\"fs.s3a.connection.ssl.enabled\",  \"true\")\n",
    "conf.set(\"fs.s3a.impl\",                    \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "\n",
    "conf.set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "conf.set(\"spark.sql.catalog.db2catalog.type\", \"jdbc\")  # JDBC catalog type for Db2\n",
    "conf.set(\"spark.sql.catalog.db2catalog.jdbc.url\", db_host_name)  # JDBC URL for Db2\n",
    "conf.set(\"spark.sql.catalog.db2catalog.jdbc.user\", db_user_name)  # Username for Db2\n",
    "conf.set(\"spark.sql.catalog.db2catalog.jdbc.password\", db_password)  # Password for Db2\n",
    "conf.set(\"spark.sql.catalog.db2catalog.jdbc.sslConnection\", \"true\")  # Enable SSL for JDBC connection\n",
    "conf.set(\"spark.sql.catalog.db2catalog.jdbc.sslTrustStoreLocation\", \"s3a://spark-analytics-target/truststore.jks\")  # Path to the truststore file if it is ssl enabled\n",
    "conf.set(\"spark.sql.catalog.db2catalog.jdbc.sslTrustStorePassword\", \"*password*\")  # Password for the truststore\n",
    "\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder.config(conf=conf)\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "conf = sc.getConf().getAll()\n",
    "\n",
    "for config in conf:\n",
    "    print(config)\n",
    "\n",
    "\n",
    "#####required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "msg_id": "086caa53-394c-4b88-934a-8adbf0f26c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database bronze_ec created at s3a://spark-dl/bronze_ec/\n",
      "Database silver_ec created at s3a://spark-dl/silver_ec/\n",
      "Database gold_ec created at s3a://spark-dl/gold_ec/\n"
     ]
    }
   ],
   "source": [
    "def create_databases(spark):\n",
    "    # Base S3 location\n",
    "    base_s3_path = \"s3a://spark-dl/\"\n",
    "    \n",
    "    # Databases to be created with corresponding paths\n",
    "    databases = {\n",
    "        \"bronze_ec\": f\"{base_s3_path}bronze_ec/\",\n",
    "        \"silver_ec\": f\"{base_s3_path}silver_ec/\",\n",
    "        \"gold_ec\": f\"{base_s3_path}gold_ec/\"\n",
    "    }\n",
    "\n",
    "    # Loop through each database and create it if not exists\n",
    "    for db_name, db_location in databases.items():\n",
    "        try:\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS spark_catalog.{db_name} LOCATION '{db_location}'\")\n",
    "            print(f\"Database {db_name} created at {db_location}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating database {db_name} at {db_location}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "create_databases(spark)\n",
    "\n",
    "#####required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "msg_id": "7652e632-8a27-41ab-9acc-08e685eacc5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------------------------------------+------+-------------+\n",
      "|customer_id|product_id|PRODUCT_NAME                             |PRICE |product_brand|\n",
      "+-----------+----------+-----------------------------------------+------+-------------+\n",
      "|307        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|301        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|306        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|309        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|303        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|308        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|305        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Infinix      |\n",
      "|302        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|310        |201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)|299.99|Xiaomi       |\n",
      "|304        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|301        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|306        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|309        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|303        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|308        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|305        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Infinix      |\n",
      "|302        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|310        |202       |OPPO F11 Pro (Aurora Green, 128 GB)      |249.99|Xiaomi       |\n",
      "|304        |203       |REDMI Note 11 (Starburst White, 64 GB)   |199.99|Xiaomi       |\n",
      "|301        |203       |REDMI Note 11 (Starburst White, 64 GB)   |199.99|Xiaomi       |\n",
      "+-----------+----------+-----------------------------------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Customer-specific recommendations table created.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def recommend_products_and_accessories_per_customer(spark):\n",
    "    # Load real-time and historical sales data\n",
    "    realtime_sales = spark.read.format(\"delta\").load(\"s3a://spark-dl/bronze_ec/realtime_sales_new/\")\n",
    "    historical_sales = spark.read.format(\"parquet\").load(\"s3a://sparkdb2-target/bronze_hist_sales/data/\")\n",
    "    \n",
    "    # Cast relevant columns and merge data\n",
    "    realtime_sales_selected = realtime_sales.select(\n",
    "        F.col(\"sales_id\").cast(\"long\"),\n",
    "        F.col(\"product_id\").cast(\"long\"),\n",
    "        F.col(\"accessory_id\").cast(\"long\"),\n",
    "        F.col(\"customer_id\").cast(\"long\"),\n",
    "        F.col(\"quantity\").cast(\"long\"),\n",
    "        F.col(\"total_amount\").cast(\"double\")\n",
    "    )\n",
    "    historical_sales_selected = historical_sales.select(\n",
    "        F.col(\"SALES_ID\").cast(\"long\"),\n",
    "        F.col(\"PRODUCT_ID\").cast(\"long\"),\n",
    "        F.col(\"ACCESSORY_ID\").cast(\"long\"),\n",
    "        F.col(\"CUSTOMER_ID\").cast(\"long\"),\n",
    "        F.col(\"QUANTITY\").cast(\"long\"),\n",
    "        F.col(\"TOTAL_AMOUNT\").cast(\"double\")\n",
    "    )\n",
    "    \n",
    "    # Combine real-time and historical sales data\n",
    "    combined_sales = realtime_sales_selected.union(historical_sales_selected)\n",
    "    \n",
    "    # Step 1: Get each customer's purchase history (both products and accessories)\n",
    "    customer_purchase_data = combined_sales.select(\"customer_id\", \"product_id\", \"accessory_id\", \"quantity\", \"total_amount\")\n",
    "    \n",
    "    # Step 2: Calculate Average Price for Each Customer per Product\n",
    "    customer_avg_price_df = customer_purchase_data.groupBy(\"customer_id\", \"product_id\").agg(\n",
    "        F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\")\n",
    "    )\n",
    "    \n",
    "    # Calculate average price spent by each customer per product\n",
    "    customer_avg_price_df = customer_avg_price_df.withColumn(\n",
    "        \"avg_price\", F.col(\"total_spent\") / F.col(\"total_quantity\")\n",
    "    )\n",
    "    \n",
    "    # Step 3: Identify the most frequent brands purchased by each customer\n",
    "    customer_brand_preference = customer_purchase_data.join(\n",
    "        spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_host_name) \\\n",
    "            .option(\"user\", db_user_name) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"dbtable\", \"ECOMMERCE.PRODUCTS\") \\\n",
    "            .option(\"sslConnection\", \"true\") \\\n",
    "            .load(),\n",
    "        \"product_id\"\n",
    "    ).groupBy(\"customer_id\", \"brand\").agg(\n",
    "        F.count(\"*\").alias(\"purchase_count\")\n",
    "    )\n",
    "    \n",
    "    # Rename 'brand' columns to avoid ambiguity\n",
    "    customer_brand_preference = customer_brand_preference.withColumnRenamed(\"brand\", \"customer_brand\")\n",
    "    \n",
    "    # Step 4: Rank brands for each customer based on purchase frequency\n",
    "    customer_brand_preference = customer_brand_preference.withColumn(\n",
    "        \"rank\", F.row_number().over(\n",
    "            Window.partitionBy(\"customer_id\").orderBy(F.desc(\"purchase_count\"))\n",
    "        )\n",
    "    ).filter(F.col(\"rank\") == 1)  # Keep only the top brand\n",
    "    \n",
    "    # Step 5: Filter products based on price range and brand preference\n",
    "    recommendations = customer_avg_price_df.join(\n",
    "        customer_brand_preference,\n",
    "        \"customer_id\",\n",
    "        \"left_outer\"  # This will handle first-time buyers who have no brand preference\n",
    "    ).join(\n",
    "        spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_host_name) \\\n",
    "            .option(\"user\", db_user_name) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"dbtable\", \"ECOMMERCE.PRODUCTS\") \\\n",
    "            .option(\"sslConnection\", \"true\") \\\n",
    "            .load(),\n",
    "        \"product_id\"\n",
    "    ).withColumnRenamed(\"brand\", \"product_brand\")  # Rename the 'brand' column in products table\n",
    "    \n",
    "    # Step 6: Handle first-time buyers with no brand preference (left join)\n",
    "    recommendations = recommendations.withColumn(\n",
    "        \"product_brand\",\n",
    "        F.coalesce(F.col(\"customer_brand\"), F.col(\"product_brand\"))  # If no brand preference, use product's brand\n",
    "    )\n",
    "    \n",
    "    # Step 7: Filter by price range for first-time buyers or those with a preference\n",
    "    recommendations = recommendations.filter(\n",
    "        (F.col(\"avg_price\") * 0.8 <= F.col(\"PRICE\")) &\n",
    "        (F.col(\"avg_price\") * 1.2 >= F.col(\"PRICE\"))\n",
    "    ).select(\n",
    "        \"customer_id\", \"product_id\", \"PRODUCT_NAME\", \"PRICE\", \"product_brand\"\n",
    "    )\n",
    "    \n",
    "    # For first-time buyers, if there's no previous data, recommend popular products in general\n",
    "    first_time_buyers = recommendations.filter(F.col(\"customer_id\").isNull())\n",
    "    \n",
    "    if first_time_buyers.count() > 0:\n",
    "        # For first-time buyers, recommend popular products based on product sales\n",
    "        popular_products = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", db_host_name) \\\n",
    "            .option(\"user\", db_user_name) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"dbtable\", \"ECOMMERCE.PRODUCTS\") \\\n",
    "            .option(\"sslConnection\", \"true\") \\\n",
    "            .load()\n",
    "        \n",
    "        # Add logic here to select popular products based on sales volume or customer ratings\n",
    "        # For example, top-selling products or those with the highest ratings\n",
    "        popular_products = popular_products.orderBy(F.desc(\"SALES_COUNT\")).limit(10)\n",
    "        \n",
    "        recommendations = recommendations.union(popular_products)  # Combine popular products with other recommendations\n",
    "\n",
    "    # Step 8: Display final recommendations\n",
    "    recommendations.show(truncate=False)\n",
    "    \n",
    "    # Optionally, save recommendations to a table for each customer\n",
    "    recommendations.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(\"gold_ec.customer_product_recommendations\")\n",
    "    \n",
    "    print(\"Customer-specific recommendations table created.\")\n",
    "    \n",
    "    # Call the recommendation function\n",
    "recommend_products_and_accessories_per_customer(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-time sales data loaded\n",
      "Historical sales data loaded\n",
      "Promotion data loaded\n",
      "Product data loaded from JDBC\n",
      "+----------+-------------------------------------------------------+-------------------+-----------------+--------------+------------+----------------+\n",
      "|PRODUCT_ID|PRODUCT_NAME                                           |sale_month         |total_sales      |total_quantity|avg_discount|promotion_active|\n",
      "+----------+-------------------------------------------------------+-------------------+-----------------+--------------+------------+----------------+\n",
      "|223       |vivo X80 Pro (Cosmic Black, 256 GB)                    |2024-11-01 00:00:00|3999.95          |5             |0.0         |0               |\n",
      "|219       |Xiaomi 11T Pro 5G Hyperphone (Celestial Magic, 256 GB) |2024-12-01 00:00:00|8166.120000000002|27            |0.0         |0               |\n",
      "|210       |SAMSUNG Galaxy A04 (Green, 128 GB)                     |2024-05-01 00:00:00|999.95           |5             |0.0         |0               |\n",
      "|218       |A10E                                                   |2024-11-01 00:00:00|5506.77          |13            |0.0         |0               |\n",
      "|201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)              |2024-09-01 00:00:00|6450.780000000001|25            |0.0         |0               |\n",
      "|225       |OPPO Ravichandran Ashwin Limited Edition (Black, 64 GB)|2024-12-01 00:00:00|9862.52          |33            |0.0         |0               |\n",
      "|217       |APPLE iPhone 14 Plus ((PRODUCT)RED, 256 GB)            |2024-11-01 00:00:00|304.76           |4             |0.0         |0               |\n",
      "|211       |vivo Z1Pro (Sonic Black, 64 GB)                        |2024-12-01 00:00:00|3112.75          |16            |0.0         |0               |\n",
      "|219       |Xiaomi 11T Pro 5G Hyperphone (Celestial Magic, 256 GB) |2024-11-01 00:00:00|1719.61          |6             |0.0         |0               |\n",
      "|209       |Infinix Note 10 Pro (Nordic Secret, 256 GB)            |2024-11-01 00:00:00|897.02           |2             |0.0         |0               |\n",
      "|207       |REDMI 10 Power (Sporty Orange, 128 GB)                 |2024-10-01 00:00:00|75335.48999999999|276           |0.0         |0               |\n",
      "|207       |REDMI 10 Power (Sporty Orange, 128 GB)                 |2024-09-01 00:00:00|4432.589999999999|16            |0.0         |0               |\n",
      "|217       |APPLE iPhone 14 Plus ((PRODUCT)RED, 256 GB)            |2024-12-01 00:00:00|3965.96          |11            |0.0         |0               |\n",
      "|205       |APPLE iPhone 13 mini (Blue, 128 GB)                    |2024-09-01 00:00:00|11728.03         |39            |0.0         |0               |\n",
      "|228       |OnePlus 9R 5G (Carbon Black, 128 GB)                   |2024-12-01 00:00:00|4219.110000000001|16            |0.0         |0               |\n",
      "|221       |Mi 10T Pro (Cosmic Black, 128 GB)                      |2024-10-01 00:00:00|2999.95          |5             |0.0         |0               |\n",
      "|226       |OnePlus 10T 5G (Jade Green, 128 GB)                    |2024-12-01 00:00:00|8769.939999999999|30            |0.0         |0               |\n",
      "|201       |REDMI Note 12 Pro 5G (Onyx Black, 128 GB)              |2024-01-01 00:00:00|1499.95          |5             |0.0         |0               |\n",
      "|206       |APPLE iPhone 6s Plus (Gold, 64 GB)                     |2024-09-01 00:00:00|6294.290000000001|23            |0.0         |0               |\n",
      "|204       |OnePlus Nord CE 5G (Blue Void, 256 GB)                 |2024-12-01 00:00:00|6554.169999999999|20            |0.0         |0               |\n",
      "+----------+-------------------------------------------------------+-------------------+-----------------+--------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Aggregated product sales data table created in Gold layer.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def product_Sales_Data(spark):\n",
    "    # Load real-time sales data\n",
    "    realtime_sales = spark.read.format(\"delta\").load(\"s3a://spark-dl/bronze_ec/realtime_sales_new/\")\n",
    "    print(\"Real-time sales data loaded\")\n",
    "\n",
    "    # Load historical sales data\n",
    "    historical_sales = spark.read.format(\"parquet\").load(\"s3a://sparkdb2-target/bronze_hist_sales/data/\")\n",
    "    print(\"Historical sales data loaded\")\n",
    "\n",
    "    # Load promotion data\n",
    "    promotion_df = spark.read.format(\"parquet\").load(\"s3a://sparkdb2-target/bronze_promotions_sale/data/\")\n",
    "    print(\"Promotion data loaded\")\n",
    "\n",
    "    product_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", db_host_name) \\\n",
    "        .option(\"user\", db_user_name) \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .option(\"dbtable\", \"ecommerce.products\") \\\n",
    "        .option(\"sslConnection\", \"true\") \\\n",
    "        .load()\n",
    "\n",
    "    product_df.createOrReplaceTempView(\"product_data\")  # Create a temporary view for product data\n",
    "    print(\"Product data loaded from JDBC\")\n",
    "\n",
    "    # Select and cast columns for real-time and historical sales\n",
    "    realtime_sales_selected = realtime_sales.select(\n",
    "        F.col(\"sales_id\").cast(\"long\").alias(\"sales_id\"),\n",
    "        F.col(\"product_id\").cast(\"long\").alias(\"product_id\"),\n",
    "        F.col(\"quantity\").cast(\"long\").alias(\"quantity\"),\n",
    "        F.col(\"total_amount\").cast(\"double\").alias(\"total_amount\"),\n",
    "        F.col(\"sale_date\").cast(\"timestamp\").alias(\"sale_date\")\n",
    "    )\n",
    "\n",
    "    historical_sales_selected = historical_sales.select(\n",
    "        F.col(\"SALES_ID\").cast(\"long\").alias(\"sales_id\"),\n",
    "        F.col(\"PRODUCT_ID\").cast(\"long\").alias(\"product_id\"),\n",
    "        F.col(\"QUANTITY\").cast(\"long\").alias(\"quantity\"),\n",
    "        F.col(\"TOTAL_AMOUNT\").cast(\"double\").alias(\"total_amount\"),\n",
    "        F.col(\"SALE_DATE\").cast(\"timestamp\").alias(\"sale_date\")\n",
    "    )\n",
    "\n",
    "    # Select promotion columns\n",
    "    promotion_selected = promotion_df.select(\n",
    "        F.col(\"Promotion_ID\"),\n",
    "        F.col(\"Product_ID\").alias(\"promotion_product_id\"),\n",
    "        F.col(\"Discount_Percentage\").cast(\"double\").alias(\"Discount_Percentage\"),\n",
    "        F.col(\"Promotion_Start_Date\"),\n",
    "        F.col(\"Promotion_End_Date\"),\n",
    "        F.col(\"Promotion_Type\")\n",
    "    )\n",
    "\n",
    "    # Combine real-time and historical sales\n",
    "    combined_sales = realtime_sales_selected.union(historical_sales_selected)\n",
    "\n",
    "    # Add is_promotion_active column\n",
    "    combined_sales_with_promo = combined_sales.join(\n",
    "        promotion_selected,\n",
    "        (combined_sales.product_id == promotion_selected.promotion_product_id) &  \n",
    "        (combined_sales.sale_date.between(promotion_selected.Promotion_Start_Date, promotion_selected.Promotion_End_Date)),\n",
    "        \"left\"\n",
    "    ).withColumn(\n",
    "        \"is_promotion_active\", \n",
    "        F.when(F.col(\"Discount_Percentage\").isNotNull(), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Handle nulls in sales and promotion features\n",
    "    combined_sales_with_promo = combined_sales_with_promo.na.fill(0.0, subset=['total_amount', 'quantity', 'Discount_Percentage'])\n",
    "\n",
    "    # Create a temporary view for combined sales with promotions\n",
    "    combined_sales_with_promo.createOrReplaceTempView(\"combined_sales_with_promo\")\n",
    "\n",
    "    # Now left join with product data to include all products\n",
    "    combined_sales_with_products = product_df.join(\n",
    "        combined_sales_with_promo,\n",
    "        product_df[\"PRODUCT_ID\"] == combined_sales_with_promo[\"product_id\"],\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        product_df[\"PRODUCT_ID\"],              # Product ID\n",
    "        product_df[\"PRODUCT_NAME\"],            # Product Name\n",
    "        F.date_trunc('month', F.col('sale_date')).alias('sale_month'),  # Sale Month\n",
    "        F.coalesce(combined_sales_with_promo.total_amount, F.lit(0)).alias('total_sales'),  # Total Sales\n",
    "        F.coalesce(combined_sales_with_promo.quantity, F.lit(0)).alias('total_quantity'),   # Total Quantity\n",
    "        F.coalesce(combined_sales_with_promo.Discount_Percentage, F.lit(0)).alias('avg_discount'),  # Average Discount\n",
    "        F.coalesce(combined_sales_with_promo.is_promotion_active, F.lit(0)).alias('promotion_active')  # Promotion Active\n",
    "    )\n",
    "\n",
    "    # Aggregate to remove duplicates by summing total_sales and total_quantity, averaging the discount, and keeping the max promotion_active flag\n",
    "    combined_sales_with_products_agg = combined_sales_with_products.groupBy(\n",
    "        \"PRODUCT_ID\", \n",
    "        \"PRODUCT_NAME\", \n",
    "        \"sale_month\"\n",
    "    ).agg(\n",
    "        F.sum(\"total_sales\").alias(\"total_sales\"),\n",
    "        F.sum(\"total_quantity\").alias(\"total_quantity\"),\n",
    "        F.avg(\"avg_discount\").alias(\"avg_discount\"),\n",
    "        F.max(\"promotion_active\").alias(\"promotion_active\")  # 1 if active in any row, otherwise 0\n",
    "    )\n",
    "\n",
    "    # Write the aggregated output to a Delta table in the Gold layer\n",
    "    combined_sales_with_products_agg.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(\"gold_ec.product_sales_data\")\n",
    "    \n",
    "    combined_sales_with_products_agg.show(truncate=False)\n",
    "\n",
    "    print(\"Aggregated product sales data table created in Gold layer.\")\n",
    "\n",
    "product_Sales_Data(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading customer segmentation data from Silver layer...\n",
      "Loading real-time sales data from Bronze layer...\n",
      "Loading product data from DB2...\n",
      "Processing High Value Customers...\n",
      "Generating promotional messages for segment with message: 'Enjoy 20% off on our premium range of products!'\n",
      "High Value Customers promotion generated.\n",
      "Processing Medium Value Customers...\n",
      "Generating promotional messages for segment with message: 'Get a special 10% discount on your next purchase!'\n",
      "Medium Value Customers promotion generated.\n",
      "Processing Low Value Customers...\n",
      "Generating promotional messages for segment with message: 'Welcome back! Enjoy 5% off your next order!'\n",
      "Low Value Customers promotion generated.\n",
      "Creating DataFrame for promotional messages...\n",
      "Removing duplicates from promotional messages...\n",
      "Showing the final promotional messages data without duplicates:\n",
      "+-----------+---------------+-------------------------------------------------+\n",
      "|CUSTOMER_ID|CUSTOMER_NAME  |PROMOTIONAL_MESSAGE                              |\n",
      "+-----------+---------------+-------------------------------------------------+\n",
      "|305        |Charlie Davis  |Enjoy 20% off on our premium range of products!  |\n",
      "|302        |Jane Smith     |Welcome back! Enjoy 5% off your next order!      |\n",
      "|309        |Liam Garcia    |Get a special 10% discount on your next purchase!|\n",
      "|304        |Bob Brown      |Enjoy 20% off on our premium range of products!  |\n",
      "|310        |Sophia Martinez|Enjoy 20% off on our premium range of products!  |\n",
      "|301        |John Doe       |Enjoy 20% off on our premium range of products!  |\n",
      "|308        |Olivia Martinez|Welcome back! Enjoy 5% off your next order!      |\n",
      "|307        |Emma Thompson  |Welcome back! Enjoy 5% off your next order!      |\n",
      "|303        |Alice Johnson  |Welcome back! Enjoy 5% off your next order!      |\n",
      "|306        |David Wilson   |Enjoy 20% off on our premium range of products!  |\n",
      "|308        |Olivia Martinez|Enjoy 20% off on our premium range of products!  |\n",
      "|302        |Jane Smith     |Enjoy 20% off on our premium range of products!  |\n",
      "|303        |Alice Johnson  |Enjoy 20% off on our premium range of products!  |\n",
      "|307        |Emma Thompson  |Enjoy 20% off on our premium range of products!  |\n",
      "|309        |Liam Garcia    |Enjoy 20% off on our premium range of products!  |\n",
      "+-----------+---------------+-------------------------------------------------+\n",
      "\n",
      "Saving distinct promotional messages to Gold layer...\n",
      "Promotional messages table created in Gold layer.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def process_promotions(spark):\n",
    "    print(\"Loading customer segmentation data from Silver layer...\")\n",
    "    customer_segmentation = spark.read \\\n",
    "        .format(\"parquet\") \\\n",
    "        .load(\"s3a://sparkdb2-target/silver_ecom/customer_segmentation/data/\")\n",
    "\n",
    "    print(\"Loading real-time sales data from Bronze layer...\")\n",
    "    real_time_sales = spark.read \\\n",
    "        .format(\"delta\") \\\n",
    "        .load(\"s3a://spark-dl/bronze_ec/realtime_sales_new/\")\n",
    "\n",
    "    print(\"Loading product data from DB2...\")\n",
    "    product_data = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", db_host_name) \\\n",
    "        .option(\"user\", db_user_name) \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .option(\"dbtable\", \"ecommerce.products\") \\\n",
    "        .option(\"sslConnection\", \"true\") \\\n",
    "        .load()\n",
    "\n",
    "    def generate_promotional_messages(segment_df, message_template):\n",
    "        print(f\"Generating promotional messages for segment with message: '{message_template}'\")\n",
    "        messages = []\n",
    "        for row in segment_df.collect():\n",
    "            messages.append((row['CUSTOMER_ID'], row['CUSTOMER_NAME'], message_template))\n",
    "        return messages\n",
    "\n",
    "    promotional_messages = []\n",
    "\n",
    "    def process_customer_segment(segment_name, discount_message):\n",
    "        print(f\"Processing {segment_name} Customers...\")\n",
    "        segment_customers = customer_segmentation.filter(customer_segmentation['SEGMENT'] == segment_name)\n",
    "        purchase_history = segment_customers.join(real_time_sales, segment_customers['CUSTOMER_ID'] == real_time_sales['customer_id'])\n",
    "        top_products = purchase_history.groupBy('product_id').agg(\n",
    "            F.sum('total_amount').alias('total_spent')\n",
    "        ).orderBy('total_spent', ascending=False).limit(10)\n",
    "\n",
    "        if not top_products.isEmpty():\n",
    "            promotional_messages.extend(generate_promotional_messages(segment_customers, discount_message))\n",
    "            print(f\"{segment_name} Customers promotion generated.\")\n",
    "\n",
    "    # Process promotions for each segment\n",
    "    process_customer_segment(\"High Value\", \"Enjoy 20% off on our premium range of products!\")\n",
    "    process_customer_segment(\"Medium Value\", \"Get a special 10% discount on your next purchase!\")\n",
    "    process_customer_segment(\"Low Value\", \"Welcome back! Enjoy 5% off your next order!\")\n",
    "\n",
    "    print(\"Creating DataFrame for promotional messages...\")\n",
    "    promotional_messages_df = spark.createDataFrame(promotional_messages, [\"CUSTOMER_ID\", \"CUSTOMER_NAME\", \"PROMOTIONAL_MESSAGE\"])\n",
    "\n",
    "    print(\"Removing duplicates from promotional messages...\")\n",
    "    promotional_messages_df = promotional_messages_df.distinct()\n",
    "\n",
    "    print(\"Showing the final promotional messages data without duplicates:\")\n",
    "    promotional_messages_df.show(truncate=False)\n",
    "\n",
    "    print(\"Saving distinct promotional messages to Gold layer...\")\n",
    "    promotional_messages_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(\"gold_ec.promotional_messages\")\n",
    "\n",
    "    print(\"Promotional messages table created in Gold layer.\")\n",
    "\n",
    "process_promotions(spark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "msg_id": "23af9c02-cb77-4169-b889-62ef799a9a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical sales data loaded\n",
      "Current sales data loaded\n",
      "+----------+-------------------+------------------+--------------+\n",
      "|product_id|sale_month         |total_sales       |total_quantity|\n",
      "+----------+-------------------+------------------+--------------+\n",
      "|202       |2024-01-01 00:00:00|1249.95           |5             |\n",
      "|201       |2024-01-01 00:00:00|1499.95           |5             |\n",
      "|null      |2024-01-01 00:00:00|174.85            |15            |\n",
      "|null      |2024-02-01 00:00:00|89.85             |15            |\n",
      "|203       |2024-02-01 00:00:00|999.95            |5             |\n",
      "|205       |2024-03-01 00:00:00|3499.9500000000003|5             |\n",
      "|206       |2024-03-01 00:00:00|1999.95           |5             |\n",
      "|204       |2024-03-01 00:00:00|1749.95           |5             |\n",
      "|null      |2024-04-01 00:00:00|29.95             |5             |\n",
      "|208       |2024-04-01 00:00:00|649.95            |5             |\n",
      "|207       |2024-04-01 00:00:00|1999.9            |10            |\n",
      "|210       |2024-05-01 00:00:00|999.95            |5             |\n",
      "|null      |2024-05-01 00:00:00|249.95            |5             |\n",
      "|209       |2024-05-01 00:00:00|2499.9            |10            |\n",
      "|211       |2024-06-01 00:00:00|749.95            |5             |\n",
      "|null      |2024-06-01 00:00:00|664.5999999999999 |40            |\n",
      "|null      |2024-07-01 00:00:00|224.89999999999998|10            |\n",
      "|212       |2024-07-01 00:00:00|499.94999999999993|5             |\n",
      "|213       |2024-07-01 00:00:00|649.95            |5             |\n",
      "|215       |2024-08-01 00:00:00|2499.95           |5             |\n",
      "+----------+-------------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE (Root Mean Squared Error) for sales prediction: 2446.082018764464\n",
      "R² (Coefficient of Determination) for sales prediction: 0.9874095757712241\n",
      "RMSE (Root Mean Squared Error) for quantity prediction: 8.97696386365264\n",
      "R² (Coefficient of Determination) for quantity prediction: 0.9871793778652901\n",
      "+----------+----------+------------------+-------------------+\n",
      "|product_id|sale_month|prediction_sales  |prediction_quantity|\n",
      "+----------+----------+------------------+-------------------+\n",
      "|201       |2025-01-02|764.1652590019032 |2.730573520121456  |\n",
      "|201       |2025-01-02|764.1652590019032 |251.66996315075306 |\n",
      "|201       |2025-01-02|764.1652590019032 |9.96586236483445   |\n",
      "|201       |2025-01-02|764.1652590019032 |24.500772109919748 |\n",
      "|201       |2025-01-02|764.1652590019032 |6.05855606360893   |\n",
      "|201       |2025-01-02|62494.130315291375|2.730573520121456  |\n",
      "|201       |2025-01-02|62494.130315291375|251.66996315075306 |\n",
      "|201       |2025-01-02|62494.130315291375|9.96586236483445   |\n",
      "|201       |2025-01-02|62494.130315291375|24.500772109919748 |\n",
      "|201       |2025-01-02|62494.130315291375|6.05855606360893   |\n",
      "|201       |2025-01-02|2626.707308114085 |2.730573520121456  |\n",
      "|201       |2025-01-02|2626.707308114085 |251.66996315075306 |\n",
      "|201       |2025-01-02|2626.707308114085 |9.96586236483445   |\n",
      "|201       |2025-01-02|2626.707308114085 |24.500772109919748 |\n",
      "|201       |2025-01-02|2626.707308114085 |6.05855606360893   |\n",
      "|201       |2025-01-02|6617.868841925904 |2.730573520121456  |\n",
      "|201       |2025-01-02|6617.868841925904 |251.66996315075306 |\n",
      "|201       |2025-01-02|6617.868841925904 |9.96586236483445   |\n",
      "|201       |2025-01-02|6617.868841925904 |24.500772109919748 |\n",
      "|201       |2025-01-02|6617.868841925904 |6.05855606360893   |\n",
      "+----------+----------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Aggregated future sales and quantity predictions table created in Gold layer.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "def process_sales_data(spark):\n",
    "    # Load historical sales data (Parquet format)\n",
    "    historical_sales = spark.read.format(\"parquet\").load(\"s3a://sparkdb2-target/bronze_hist_sales/data/\")\n",
    "    print(\"Historical sales data loaded\")\n",
    "\n",
    "    # Load current sales data (Delta format)\n",
    "    current_sales = spark.read.format(\"delta\").load(\"s3a://spark-dl/bronze_ec/realtime_sales_new/\")\n",
    "    print(\"Current sales data loaded\")\n",
    "\n",
    "    # Preprocessing: Select necessary columns for sales\n",
    "    historical_sales_selected = historical_sales.select(\n",
    "        F.col(\"SALES_ID\").cast(\"long\").alias(\"sales_id\"),\n",
    "        F.col(\"PRODUCT_ID\").cast(\"long\").alias(\"product_id\"),\n",
    "        F.col(\"QUANTITY\").cast(\"long\").alias(\"quantity\"),\n",
    "        F.col(\"TOTAL_AMOUNT\").cast(\"double\").alias(\"total_amount\"),\n",
    "        F.col(\"SALE_DATE\").cast(\"timestamp\").alias(\"sale_date\")\n",
    "    )\n",
    "\n",
    "    current_sales_selected = current_sales.select(\n",
    "        F.col(\"sales_id\").cast(\"long\").alias(\"sales_id\"),\n",
    "        F.col(\"product_id\").cast(\"long\").alias(\"product_id\"),\n",
    "        F.col(\"quantity\").cast(\"long\").alias(\"quantity\"),\n",
    "        F.col(\"total_amount\").cast(\"double\").alias(\"total_amount\"),\n",
    "        F.col(\"sale_date\").cast(\"timestamp\").alias(\"sale_date\")\n",
    "    )\n",
    "\n",
    "    # Fill missing values in historical and current sales data\n",
    "    historical_sales_selected = historical_sales_selected.fillna({\"quantity\": 0, \"total_amount\": 0.0})\n",
    "    current_sales_selected = current_sales_selected.fillna({\"quantity\": 0, \"total_amount\": 0.0})\n",
    "\n",
    "    # Combine the datasets\n",
    "    combined_sales = historical_sales_selected.union(current_sales_selected)\n",
    "\n",
    "    # Aggregate sales data by month (both historical and current)\n",
    "    combined_sales_monthly = combined_sales.withColumn(\n",
    "        \"sale_month\", F.date_trunc('month', F.col('sale_date'))\n",
    "    ).groupBy(\n",
    "        'product_id', 'sale_month'\n",
    "    ).agg(\n",
    "        F.sum('total_amount').alias('total_sales'),\n",
    "        F.sum('quantity').alias('total_quantity')\n",
    "    ).orderBy('sale_month')\n",
    "\n",
    "    # Fill missing values in the aggregated columns (total_sales, total_quantity)\n",
    "    combined_sales_monthly = combined_sales_monthly.fillna({\"total_sales\": 0.0, \"total_quantity\": 0})\n",
    "\n",
    "    # Show the aggregated data\n",
    "    combined_sales_monthly.show(truncate=False)\n",
    "\n",
    "    # Feature engineering for total_sales prediction: Prepare the features for linear regression (using total_quantity)\n",
    "    assembler_sales = VectorAssembler(\n",
    "        inputCols=['total_quantity'],  # Using only quantity to predict sales\n",
    "        outputCol='features_sales'\n",
    "    )\n",
    "\n",
    "    combined_sales_with_features_sales = assembler_sales.transform(combined_sales_monthly)\n",
    "\n",
    "    # Feature engineering for total_quantity prediction: Prepare the features for linear regression (using total_sales)\n",
    "    assembler_quantity = VectorAssembler(\n",
    "        inputCols=['total_sales'],  # Using sales to predict quantity\n",
    "        outputCol='features_quantity'\n",
    "    )\n",
    "\n",
    "    combined_sales_with_features_quantity = assembler_quantity.transform(combined_sales_monthly)\n",
    "\n",
    "    # Split the data into training and test sets (80% training, 20% testing) for sales prediction\n",
    "    train_data_sales, test_data_sales = combined_sales_with_features_sales.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train a Linear Regression model for predicting total_sales\n",
    "    lr_sales = LinearRegression(featuresCol='features_sales', labelCol='total_sales')\n",
    "    lr_model_sales = lr_sales.fit(train_data_sales)\n",
    "\n",
    "    # Split the data into training and test sets (80% training, 20% testing) for quantity prediction\n",
    "    train_data_quantity, test_data_quantity = combined_sales_with_features_quantity.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train a Linear Regression model for predicting total_quantity\n",
    "    lr_quantity = LinearRegression(featuresCol='features_quantity', labelCol='total_quantity')\n",
    "    lr_model_quantity = lr_quantity.fit(train_data_quantity)\n",
    "\n",
    "    # Evaluate the sales model on test data\n",
    "    test_results_sales = lr_model_sales.evaluate(test_data_sales)\n",
    "    print(f\"RMSE (Root Mean Squared Error) for sales prediction: {test_results_sales.rootMeanSquaredError}\")\n",
    "    print(f\"R² (Coefficient of Determination) for sales prediction: {test_results_sales.r2}\")\n",
    "\n",
    "    # Evaluate the quantity model on test data\n",
    "    test_results_quantity = lr_model_quantity.evaluate(test_data_quantity)\n",
    "    print(f\"RMSE (Root Mean Squared Error) for quantity prediction: {test_results_quantity.rootMeanSquaredError}\")\n",
    "    print(f\"R² (Coefficient of Determination) for quantity prediction: {test_results_quantity.r2}\")\n",
    "\n",
    "    # Predict future sales (for the next month)\n",
    "    future_months_df = combined_sales_monthly.select(\"product_id\").distinct().crossJoin(\n",
    "        spark.range(1, 2).select(F.col(\"id\").alias(\"future_month\"))\n",
    "    ).withColumn(\n",
    "        \"sale_month\", F.add_months(F.current_date(), 1)  # Predict for the next month\n",
    "    )\n",
    "\n",
    "    # Join future_months_df with combined_sales_monthly to get total_quantity and total_sales\n",
    "    future_months_with_data = future_months_df.join(\n",
    "        combined_sales_monthly.withColumnRenamed(\"sale_month\", \"historical_sale_month\"),\n",
    "        on='product_id', how='left'\n",
    "    ).select(\n",
    "        \"product_id\", \"sale_month\", \"total_quantity\", \"total_sales\"\n",
    "    )\n",
    "\n",
    "    # Fill any null values in the future predictions with 0\n",
    "    future_months_with_data = future_months_with_data.fillna({\"total_quantity\": 0, \"total_sales\": 0.0})\n",
    "\n",
    "    # Add features for prediction (sales and quantity predictions)\n",
    "    future_months_with_features_sales = assembler_sales.transform(future_months_with_data)\n",
    "    future_months_with_features_quantity = assembler_quantity.transform(future_months_with_data)\n",
    "\n",
    "    # Generate predictions for total_sales\n",
    "    future_predictions_sales = lr_model_sales.transform(future_months_with_features_sales).withColumnRenamed(\n",
    "        \"prediction\", \"prediction_sales\"\n",
    "    )\n",
    "\n",
    "    # Generate predictions for total_quantity\n",
    "    future_predictions_quantity = lr_model_quantity.transform(future_months_with_features_quantity).withColumnRenamed(\n",
    "        \"prediction\", \"prediction_quantity\"\n",
    "    )\n",
    "\n",
    "    # Combine the predictions into a single DataFrame\n",
    "    future_predictions_combined = future_predictions_sales.join(\n",
    "        future_predictions_quantity,\n",
    "        on=[\"product_id\", \"sale_month\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Show the predictions for total_sales and total_quantity\n",
    "    future_predictions_combined.select(\"product_id\", \"sale_month\", \"prediction_sales\", \"prediction_quantity\").show(truncate=False)\n",
    "\n",
    "    # Write the combined future prediction output to a Delta table in the Gold layer\n",
    "    future_predictions_combined.select('product_id', 'sale_month', 'prediction_sales', 'prediction_quantity') \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(\"gold_ec.aggregated_sales_and_quantity_predictions\")\n",
    "\n",
    "    print(\"Aggregated future sales and quantity predictions table created in Gold layer.\")\n",
    "\n",
    "# Call the function to run the processing\n",
    "process_sales_data(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 with Spark",
   "language": "python3",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
